services:
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.32.0
    restart: unless-stopped
    ports:
      - "8080:8080"
      - "50051:50051"
    volumes:
      - ./weaviate_data:/var/lib/weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      DEFAULT_VECTORIZER_MODULE: 'none'
      CLUSTER_HOSTNAME: 'node1'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,ref2vec-centroid,generative-openai,qna-openai'
      PERSISTENCE_LSM_ACCESS_STRATEGY_DISK_USE_READONLY_PERCENTAGE: 90
      TZ: Asia/Kolkata
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/v1/.well-known/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:8.2.1-alpine
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # Ollama (internal-only). CPU by default; add GPU with docker-compose.gpu.yml.
  ollama:
    image: ollama/ollama:0.3.14
    restart: unless-stopped
    # No published port; only containers on this network can reach it.
    # ports:
    #   - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=1
    volumes:
      - ollama_data:/root/.ollama
    healthcheck:
      # CLI talks to the local server; works without curl
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  backend:
    build: .
    restart: unless-stopped
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - WEAVIATE_HOST=weaviate
      - REDIS_HOST=redis
      - TZ=Asia/Kolkata
      - OLLAMA_HOST=http://ollama:11434
      # Avoid overriding COMPRESSION_LLM_PROVIDER here; prefer .env/config.py
    volumes:
      - ./data:/app/data
      - ./src/agentic_rag:/app/agentic_rag
    depends_on:
      weaviate:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy
    command: ["uvicorn", "agentic_rag.app.api:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  ingestion-worker:
    build: .
    restart: "no"
    env_file:
      - .env
    environment:
      - WEAVIATE_HOST=weaviate
      - TZ=Asia/Kolkata
    volumes:
      - ./data:/app/data
      - ./src/agentic_rag:/app/agentic_rag
    depends_on:
      weaviate:
        condition: service_healthy
    command: ["python", "-m", "agentic_rag.scripts.run_ingestion"]

volumes:
  redis_data:
  ollama_data: